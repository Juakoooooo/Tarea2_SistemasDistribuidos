# ğŸ›’ Proyecto: Sistema Distribuido para la GestiÃ³n LogÃ­stica en una AplicaciÃ³n de Ecommerce

Este proyecto implementa un sistema distribuido no monolÃ­tico para gestionar el estado de los pedidos en una aplicaciÃ³n de ecommerce, utilizando Apache Kafka como sistema de mensajerÃ­a, gRPC para la comunicaciÃ³n interna, y Elasticsearch para el almacenamiento y anÃ¡lisis de mÃ©tricas de rendimiento.

## ğŸ› ï¸ Requisitos del Sistema

- ğŸ‹ Docker y Docker Compose
- ğŸ Python 3.8 o superior

## ğŸ“¦ InstalaciÃ³n de Dependencias

Ejecuta los siguientes comandos para instalar las dependencias necesarias:

```bash
sudo pip install grpcio grpcio-tools confluent-kafka elasticsearch
```

### âš™ï¸ Configuracion del entorno

1. Clonamos el repositorio y nos dirigimos a la carpeta root del repositorio.

    ```bash
    sudo git clone https://github.com/Juakoooooo/Tarea2_SistemasDistribuidos.git
    cd Tarea2_SistemasDistribuidos
    ```

2. Levantamos los contenedores.

    ```bash
    sudo docker-compose up -d
    ```

3. Si queremos detener los contenedores, recomendamos usar este comando para evitar errores con los volÃºmenes de kafka

    ```bash
    sudo docker compose down --volumes --rmi all --remove-orphans
    ```

### ğŸš€ EjecuciÃ³n del proyecto

Para ejecutar el proyecto, abre tres terminales separadas y sigue los pasos en el orden indicado:

#### 1. Ejecutar el servidor gRPC ğŸ–¥ï¸

En la primera terminal, inicia el servidor gRPC:

```bash
cd grpc_server
sudo python3 server.py
```

#### 2. Ejecutar el consumer de kafka ğŸ“¥

En la segunda terminal, inicia consumer:

```bash
cd consumer
sudo python3 consumer.py
```

#### 3. Ejecutar el cliente gRPC ğŸ“¤

En la tercera terminal, inicia el cliente gRPC:

```bash
cd grpc_client
sudo python3 client.py
```

El cliente gRPC leerÃ¡ el archivo **compras_dataset.csv** y enviarÃ¡ las solicitudes de compra al servidor gRPC, que a su vez las transmitirÃ¡ al productor de Kafka. El consumidor de Kafka procesarÃ¡ los mensajes y los almacenarÃ¡ en Elasticsearch.

## âœ‰ï¸ Pruebas de la Funcionalidad de EnvÃ­o de Correos

Por defecto, la funcionalidad de envÃ­o de correos estÃ¡ comentada en el archivo **consumer/consumer.py** para evitar el envÃ­o masivo de correos durante las pruebas. Si deseas probar esta funcionalidad, ***descomenta*** la secciÃ³n de cÃ³digo debajo de la linea que dice "*Este apartado de cÃ³digo realiza el procesamiento de los datos y envÃ­os de correos*", quedaria de tal forma:

```python
#Este apartado de cÃ³digo realiza el procesamiento de los datos y envÃ­os de correos
from confluent_kafka import Consumer
from elasticsearch import Elasticsearch
import json
import time
import random
import smtplib
from email.mime.text import MIMEText...
```

Luego, si lo deseas, puedes cambiar el correo de destino, para verificar esta funcionalidad en:

```python
compra_id = msg.key().decode('utf-8')
destinatario = "lexislema10@gmail.com" # email a cambiar
maquina_estados(compra_id, destinatario)
```

Finalmente, El nivel de carga puede ajustarse cambiando la variable carga_trabajo en el archivo **consumer/consumer.py**. La carga puede ser "baja", "media" o "alta", lo que afecta el tiempo de espera entre el procesamiento de mensajes. Modifica la lÃ­nea:

```python
carga_trabajo = "alta"
```

a "media" o "baja" para simular diferentes condiciones de carga.

## Kibana ğŸ“Š

Para conectarse a Kibana y visualizar las mÃ©tricas y datos relacionados con los pedidos, se deben seguir los siguientes pasos:

1. Una vez ejecutado el proyecto, nos dirigimos al localhost, en el puerto 5601.
   [**kibana**](http://localhost:5601/app/home#/)
   ![Homepage de Kibana](images/kibanahomepage.png)

2. Luego nos dirigimos al apartado de **dashboard**
   ![Dashboard](images/dashboard.png)

3. Nos pedira crear *index pattern*, lo hacemos con los dos index mandados por kafka: **compras-index** y **metrics-index**.
    ![Crear index pattern](images/createindex.png)
    ![Ãndices creados](images/indexcreated.png)

4. Luego en el dashboard, creamos una visualizaciÃ³n:
   ![VisualizaciÃ³n](images/visualizacion.png)

5. Finalmente, para consultar datos respecto a los pedidos podemos elegir algÃºn campo en particular, seleccionar el tipo de grÃ¡fico y configuraciones relacionadas con la informaciÃ³n que muestra el grÃ¡fico
   ![Grafico bancos](images/bank.png)

Por otro lado, si queremos analizar las mÃ©tricas como *latencia* o *throughput*, seleccionamos el filtro, determinamos el campo **metrica** y le asignamos el valor que nosotros queramos analizar (en este caso, latencia).
    ![Filtro de index metrics](images/metricsfilter.png)

Se mostrarÃ¡ lo siguiente (notar que se seleccionÃ³ como funcion **Average**):
    ![latencia average](images/latency.png)

## ğŸ“š Referencias

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)ğŸ“œ
- [gRPC Documentation](https://grpc.io/docs/)ğŸš€
- [Elasticsearch Documentation](https://www.elastic.co/guide/en/elasticsearch/)ğŸ”
- CÃ³digo usade de CÃ©sar MuÃ±Ã³z disponible en [consumer.py](https://github.com/cesarmunozr/SD-2024-2/blob/kafka/consumer.py)ğŸ’»
